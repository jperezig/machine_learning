{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import random\n",
    "import math\n",
    "import collections\n",
    "import itertools\n",
    "from enum import Enum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = 'text8.zip'\n",
    "VOCABULARY_SIZE = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "  \n",
    "words = read_data(FILENAME)\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling \n",
    "Drop words with probability $$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$, where t = 1e-5 and $f(w_i)$ = frequency of the term among dataset.\n",
    "\n",
    "Besides we remove those words of length == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 1e-5\n",
    "\n",
    "def subsampling(words):\n",
    "    counts = collections.Counter(words)\n",
    "    deleted = []\n",
    "    for w, c in list(counts.items()):\n",
    "        freq = c / len(words)\n",
    "        if len(w) <=2 or random.random() < 1 - math.sqrt(THRESHOLD/freq):\n",
    "            deleted.append(w)\n",
    "            del counts[w]\n",
    "    return deleted, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, with_subsampling = True):\n",
    "    dictionary = {}\n",
    "    counter = [('UNK', -1)]\n",
    "    if with_subsampling:\n",
    "        deleted, subsampled_words = subsampling(words)\n",
    "        counter.extend(subsampled_words.most_common(VOCABULARY_SIZE - 1))\n",
    "    else:\n",
    "        deleted = []\n",
    "        counter.extend(collections.Counter(words).most_common(VOCABULARY_SIZE - 1))\n",
    "    ## Build Dictionary\n",
    "    for word, _ in counter:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    unk_count = 0\n",
    "    words_idx = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count +=1\n",
    "        words_idx.append(index)\n",
    "    counter[0] = ('UNK', unk_count)\n",
    "    return deleted, counter, words_idx, dictionary, dict(zip(dictionary.values(),dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted, counts, words_idx, dictionary, reverse_dictionary = build_dataset(words, with_subsampling=True)\n",
    "print('Words:', words[:10])\n",
    "print('Deleted:', deleted[:10])\n",
    "print('Sample data:',words_idx[:10])\n",
    "print('Sample data:',[reverse_dictionary[w] for w in words_idx[:10]])\n",
    "print('Common words:', counts[:5])\n",
    "print('Reverse Dictionary:', list(reverse_dictionary.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_generator(data, batch_size, window_size):\n",
    "    batch = []\n",
    "    labels = []\n",
    "    span = window_size * 2 + 1 #[skip_window key skip_window]\n",
    "\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for word in data:\n",
    "        if word != 0:\n",
    "            buffer.append(word)\n",
    "            if len(buffer) == span:\n",
    "                labels.append(buffer[window_size])\n",
    "                batch.extend(random.sample(list(buffer)[:window_size] + list(buffer)[window_size + 1:], window_size * 2))\n",
    "                if len(labels) == batch_size:\n",
    "                    yield np.array(batch).reshape(-1, window_size * 2), np.array(labels)\n",
    "                    batch = []\n",
    "                    labels = []\n",
    "\n",
    "print ('data:', [reverse_dictionary[idx] for idx in words_idx[:50] if idx != 0])\n",
    "\n",
    "for w_s in (1,2):\n",
    "    generator = cbow_generator(words_idx, batch_size=8, window_size=w_s)\n",
    "    batch, labels = list(itertools.islice(generator, 0,1,1))[0]\n",
    "    print('\\nwith window_size =',  w_s)\n",
    "    shape = batch.shape\n",
    "    print('-batch:\\n', np.array([reverse_dictionary[bi] for bi in batch.flat]).reshape(shape))\n",
    "    print('-labels:', [reverse_dictionary[li] for li in labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def skipgram_generator(data, batch_size, ngram_size, window_size):\n",
    "    assert batch_size % ngram_size == 0\n",
    "    assert ngram_size <= 2 * window_size\n",
    "    batch = []\n",
    "    labels = []\n",
    "\n",
    "    span = window_size * 2 + 1 #[skip_window key skip_window]\n",
    "    #Fill the buffer until span\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for word in data:\n",
    "        if word != 0:\n",
    "            buffer.append(word)\n",
    "            if len(buffer) ==  span:\n",
    "                batch.extend([buffer[window_size]] * ngram_size)\n",
    "                labels.extend(random.sample(list(buffer)[:window_size] + list(buffer)[window_size + 1:], ngram_size))\n",
    "                if len(batch) == batch_size:\n",
    "                    yield np.array(batch, dtype=np.int32), np.array(labels, dtype=np.int32)\n",
    "                    batch = []\n",
    "                    labels = []\n",
    "\n",
    "print('data:',[reverse_dictionary[w] for w in words_idx[:50] if w != 0] )\n",
    "\n",
    "for ngram_size, window_size in [(2,1),(1,2), (2, 4)]:\n",
    "    generator = skipgram_generator(words_idx, batch_size=8, ngram_size=ngram_size, window_size=window_size)\n",
    "    batch, labels = list(itertools.islice(generator, 0,1,1))[0]\n",
    "    print('\\nwith ngram_size = %d and window_size = %d:' % (ngram_size, window_size))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Enum):\n",
    "    CBOW = 1\n",
    "    SKIPGRAM = 0\n",
    "\n",
    "NGRAM_SIZE = 2\n",
    "WINDOW_SIZE = 1\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDINGS_SIZE =  100\n",
    "NEGATIVE_SAMPLE = 50\n",
    "NUM_VALIDATION_WORDS = 15\n",
    "TOP_K_RELATED_WORDS = 8\n",
    "MODEL = Model.CBOW\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    #Input Data\n",
    "    if MODEL == Model.CBOW:\n",
    "        inputs = tf.placeholder(shape=(BATCH_SIZE, WINDOW_SIZE * 2), dtype=tf.int32, name='inputs')\n",
    "    else:\n",
    "        inputs = tf.placeholder(shape=(BATCH_SIZE), dtype=tf.int32, name='inputs')\n",
    "    \n",
    "    labels = tf.placeholder(shape=(BATCH_SIZE,1), dtype=tf.int32, name='labels')\n",
    "    \n",
    "    \n",
    "    #Embedding\n",
    "    embedding = tf.Variable(tf.random_uniform((VOCABULARY_SIZE, EMBEDDINGS_SIZE), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs) \n",
    "    \n",
    "    #Weights\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([VOCABULARY_SIZE, EMBEDDINGS_SIZE],\n",
    "                                                      stddev=1.0 / math.sqrt(EMBEDDINGS_SIZE)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([VOCABULARY_SIZE]))\n",
    "\n",
    "    \n",
    "    #Loss\n",
    "    \n",
    "    if MODEL == Model.CBOW:\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=tf.reduce_sum(embed,1), \n",
    "                                       labels=labels, num_sampled=NEGATIVE_SAMPLE, num_classes=VOCABULARY_SIZE))\n",
    "    else:\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                       labels=labels, num_sampled=NEGATIVE_SAMPLE, num_classes=VOCABULARY_SIZE))\n",
    "    \n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    #Evaluation\n",
    "    valid_examples = np.random.choice(range(1,100), NUM_VALIDATION_WORDS, replace=False)\n",
    "\n",
    "    norm_embeddings = embedding / tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    eval_words = np.random.randint(VOCABULARY_SIZE, size = 10)\n",
    "    eval_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    eval_embedding = tf.nn.embedding_lookup(norm_embeddings, eval_dataset)\n",
    "    #Compute cosine of eval_embed vectors\n",
    "    cosine = tf.matmul(eval_embedding, tf.transpose(norm_embeddings))\n",
    "    top_k = tf.nn.top_k(cosine, TOP_K_RELATED_WORDS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized using '{}'\".format(MODEL))\n",
    "    step = 0\n",
    "    epochs = 10\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if MODEL == Model.CBOW:\n",
    "            generator = cbow_generator(words_idx, batch_size=BATCH_SIZE, window_size=WINDOW_SIZE)\n",
    "        else:\n",
    "            generator = skipgram_generator(words_idx, batch_size=BATCH_SIZE, \n",
    "                                           ngram_size=NGRAM_SIZE, window_size=WINDOW_SIZE)\n",
    "        \n",
    "        for b, l in generator:\n",
    "            step +=1\n",
    "            _, ls = session.run([optimizer, loss], feed_dict={inputs:b, labels: l.reshape(-1,1)})\n",
    "            if step % 10000 == 0:\n",
    "                print('   Epoch:{}. Step:{}. Loss:{}'.format(epoch, step, ls))    \n",
    "        \n",
    "        print('Final Step:',step)\n",
    "        _, top_idx = session.run(top_k)\n",
    "        for top in top_idx:\n",
    "            print(\"   Nearest to '{}': {}\".format(reverse_dictionary[top[0]], \n",
    "                                                      [reverse_dictionary[w] for w in top[1:]]))\n",
    "        step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
