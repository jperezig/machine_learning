{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  \n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(-1,image_size, image_size, num_channels)\n",
    "    labels = (labels.reshape(-1,1) == np.arange(10)).astype(int)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, axis=1) == np.argmax(labels, axis=1))\n",
    "            / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_input(graph, batch_size):\n",
    "    Dataset = collections.namedtuple('Dataset', 'X_train y_train X_validation y_validation X_test y_test')\n",
    "    with graph.as_default():\n",
    "        X_train = tf.placeholder(shape=(batch_size, image_size, \n",
    "                                        image_size, num_channels), dtype=tf.float32, name='X_train')\n",
    "        y_train = tf.placeholder(shape=(batch_size, num_labels), dtype=tf.float32, name='y_train')\n",
    "    \n",
    "        X_validation = tf.constant(valid_dataset, dtype=tf.float32, name='X_validation')\n",
    "        y_validation = tf.constant(valid_labels, dtype=tf.float32, name='y_validation')\n",
    "    \n",
    "        X_test = tf.constant(test_dataset, dtype=tf.float32, name='X_test')\n",
    "        y_test = tf.constant(test_labels, dtype=tf.float32, name='y_test')\n",
    "    return Dataset(X_train, y_train, X_validation, y_validation, X_test, y_validation)\n",
    "\n",
    "def get_weights_biases(graph, shape, name):\n",
    "    with graph.as_default():\n",
    "\n",
    "        return tf.Variable(tf.truncated_normal(tf.TensorShape(shape), stddev=0.1), name='weights_' + name), \\\n",
    "               tf.Variable(tf.zeros(shape[-1]), name=name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes a 2-D convolution given 4-D input and filter tensors.\n",
    "\n",
    "Given an input tensor of shape [batch, in_height, in_width, in_channels] and a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels], this op performs the following:\n",
    "\n",
    "Flattens the filter to a 2-D matrix with shape [filter_height * filter_width * in_channels, output_channels].\n",
    "Extracts image patches from the input tensor to form a virtual tensor of shape [batch, out_height, out_width, filter_height * filter_width * in_channels].\n",
    "For each patch, right-multiplies the filter matrix and the image patch vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 150\n",
    "#batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "data = define_input(graph, batch_size)\n",
    "with graph.as_default():\n",
    "    \n",
    "    #Filter -> [filter_height filter_width in_channels, output_channels]\n",
    "    #Layer 1\n",
    "    layer1_weights, layer1_biases = get_weights_biases(graph, shape=(patch_size, patch_size, num_channels, depth), name='1')\n",
    "\n",
    "    #Layer 2\n",
    "    layer2_weights, layer2_biases = get_weights_biases(graph, shape=(patch_size, patch_size, depth, depth*2), name='2')\n",
    "\n",
    "    #Layer 3\n",
    "    layer3_weights, layer3_biases = get_weights_biases(graph,\n",
    "        shape=(image_size // 4 * image_size // 4 * depth*2,num_hidden), name='3')\n",
    "\n",
    "    #Layer 4\n",
    "    layer4_weights, layer4_biases = get_weights_biases(graph,shape=(num_hidden, num_labels), name='4')\n",
    "    \n",
    "    def model_pool(data):\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1,1,1,1], padding='SAME')\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        hidden1 = tf.nn.relu(pool1 + layer1_biases)\n",
    "        conv2 = tf.nn.conv2d(hidden1, layer2_weights, [1,1,1,1], padding='SAME')\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        hidden2 = tf.nn.relu(pool2 + layer2_biases)\n",
    "        hidden2_shape = hidden2.get_shape().as_list()\n",
    "        hidden2_reshape = tf.reshape(hidden2, [hidden2_shape[0], hidden2_shape[1] * hidden2_shape[2] * hidden2_shape[3]])\n",
    "        hidden3 = tf.nn.relu(tf.matmul(hidden2_reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden3, layer4_weights) + layer4_biases\n",
    "    \n",
    "    def model(data):\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1,2,2,1], padding='SAME')\n",
    "        hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "        conv2 = tf.nn.conv2d(hidden1, layer2_weights, [1,2,2,1], padding='SAME')\n",
    "        hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "        hidden2_shape = hidden2.get_shape().as_list()\n",
    "        hidden2_reshape = tf.reshape(hidden2, [hidden2_shape[0], hidden2_shape[1] * hidden2_shape[2] * hidden2_shape[3]])\n",
    "        hidden3 = tf.nn.relu(tf.matmul(hidden2_reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden3, layer4_weights) + layer4_biases\n",
    "    \n",
    "    logits = model(data.X_train)\n",
    "    \n",
    "    l2_regularizer = tf.nn.l2_loss(layer1_weights) + \\\n",
    "                     tf.nn.l2_loss(layer2_weights) + \\\n",
    "                     tf.nn.l2_loss(layer3_weights) + \\\n",
    "                     tf.nn.l2_loss(layer4_weights)\n",
    "\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=data.y_train)\n",
    "    ) + (0.001*l2_regularizer)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(loss)\n",
    "    \n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    validation_predictions = model(data.X_validation)\n",
    "    test_predictions = model(data.X_test)\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 2.79. MiniBatch Accuracy: 9.33. Validation Accuracy: 9.55\n",
      "Epoch: 100. Loss: 1.43. MiniBatch Accuracy: 73.33. Validation Accuracy: 78.03\n",
      "Epoch: 200. Loss: 1.16. MiniBatch Accuracy: 80.67. Validation Accuracy: 80.83\n",
      "Epoch: 300. Loss: 1.29. MiniBatch Accuracy: 74.67. Validation Accuracy: 81.90\n",
      "Epoch: 400. Loss: 0.95. MiniBatch Accuracy: 83.33. Validation Accuracy: 82.64\n",
      "Epoch: 500. Loss: 0.97. MiniBatch Accuracy: 85.33. Validation Accuracy: 83.09\n",
      "Epoch: 600. Loss: 1.20. MiniBatch Accuracy: 76.00. Validation Accuracy: 83.30\n",
      "Epoch: 700. Loss: 1.08. MiniBatch Accuracy: 81.33. Validation Accuracy: 84.32\n",
      "Epoch: 800. Loss: 0.98. MiniBatch Accuracy: 84.00. Validation Accuracy: 84.75\n",
      "Epoch: 900. Loss: 0.94. MiniBatch Accuracy: 84.67. Validation Accuracy: 84.91\n",
      "Epoch: 1000. Loss: 0.88. MiniBatch Accuracy: 87.33. Validation Accuracy: 85.14\n",
      "Epoch: 1100. Loss: 0.83. MiniBatch Accuracy: 87.33. Validation Accuracy: 85.39\n",
      "Epoch: 1200. Loss: 0.81. MiniBatch Accuracy: 87.33. Validation Accuracy: 85.91\n",
      "Epoch: 1300. Loss: 0.80. MiniBatch Accuracy: 88.00. Validation Accuracy: 86.17\n",
      "Epoch: 1400. Loss: 0.94. MiniBatch Accuracy: 82.67. Validation Accuracy: 86.02\n",
      "Epoch: 1500. Loss: 0.78. MiniBatch Accuracy: 89.33. Validation Accuracy: 86.41\n",
      "Epoch: 1600. Loss: 0.82. MiniBatch Accuracy: 86.00. Validation Accuracy: 86.64\n",
      "Epoch: 1700. Loss: 0.99. MiniBatch Accuracy: 84.00. Validation Accuracy: 86.72\n",
      "Epoch: 1800. Loss: 0.82. MiniBatch Accuracy: 89.33. Validation Accuracy: 86.76\n",
      "Epoch: 1900. Loss: 0.96. MiniBatch Accuracy: 86.00. Validation Accuracy: 86.74\n",
      "Epoch: 2000. Loss: 0.89. MiniBatch Accuracy: 86.00. Validation Accuracy: 87.38\n",
      "Epoch: 2100. Loss: 0.70. MiniBatch Accuracy: 92.67. Validation Accuracy: 87.42\n",
      "Epoch: 2200. Loss: 0.93. MiniBatch Accuracy: 84.00. Validation Accuracy: 87.17\n",
      "Epoch: 2300. Loss: 0.86. MiniBatch Accuracy: 85.33. Validation Accuracy: 87.73\n",
      "Epoch: 2400. Loss: 0.81. MiniBatch Accuracy: 85.33. Validation Accuracy: 87.36\n",
      "Epoch: 2500. Loss: 1.08. MiniBatch Accuracy: 79.33. Validation Accuracy: 87.79\n",
      "Epoch: 2600. Loss: 0.81. MiniBatch Accuracy: 84.67. Validation Accuracy: 87.87\n",
      "Epoch: 2700. Loss: 0.80. MiniBatch Accuracy: 88.00. Validation Accuracy: 87.74\n",
      "Epoch: 2800. Loss: 0.64. MiniBatch Accuracy: 92.67. Validation Accuracy: 88.35\n",
      "Epoch: 2900. Loss: 0.90. MiniBatch Accuracy: 84.67. Validation Accuracy: 88.14\n",
      "Epoch: 3000. Loss: 0.80. MiniBatch Accuracy: 84.67. Validation Accuracy: 88.23\n",
      "Epoch: 3100. Loss: 0.69. MiniBatch Accuracy: 88.00. Validation Accuracy: 88.04\n",
      "Epoch: 3200. Loss: 0.89. MiniBatch Accuracy: 82.00. Validation Accuracy: 88.33\n",
      "Epoch: 3300. Loss: 0.78. MiniBatch Accuracy: 86.00. Validation Accuracy: 88.38\n",
      "Epoch: 3400. Loss: 0.72. MiniBatch Accuracy: 89.33. Validation Accuracy: 88.26\n",
      "Epoch: 3500. Loss: 0.81. MiniBatch Accuracy: 86.00. Validation Accuracy: 88.50\n",
      "Epoch: 3600. Loss: 0.68. MiniBatch Accuracy: 86.67. Validation Accuracy: 88.55\n",
      "Epoch: 3700. Loss: 0.73. MiniBatch Accuracy: 88.67. Validation Accuracy: 88.60\n",
      "Epoch: 3800. Loss: 0.68. MiniBatch Accuracy: 87.33. Validation Accuracy: 88.71\n",
      "Epoch: 3900. Loss: 0.53. MiniBatch Accuracy: 93.33. Validation Accuracy: 88.91\n",
      "Epoch: 4000. Loss: 0.73. MiniBatch Accuracy: 87.33. Validation Accuracy: 88.55\n",
      "Epoch: 4100. Loss: 0.80. MiniBatch Accuracy: 86.00. Validation Accuracy: 89.03\n",
      "Epoch: 4200. Loss: 0.88. MiniBatch Accuracy: 84.67. Validation Accuracy: 88.72\n",
      "Epoch: 4300. Loss: 0.70. MiniBatch Accuracy: 86.67. Validation Accuracy: 89.04\n",
      "Epoch: 4400. Loss: 0.73. MiniBatch Accuracy: 89.33. Validation Accuracy: 88.84\n",
      "Epoch: 4500. Loss: 0.67. MiniBatch Accuracy: 90.00. Validation Accuracy: 89.10\n",
      "Epoch: 4600. Loss: 0.63. MiniBatch Accuracy: 89.33. Validation Accuracy: 89.03\n",
      "Epoch: 4700. Loss: 0.74. MiniBatch Accuracy: 86.00. Validation Accuracy: 89.13\n",
      "Epoch: 4800. Loss: 0.76. MiniBatch Accuracy: 87.33. Validation Accuracy: 89.17\n",
      "Epoch: 4900. Loss: 0.59. MiniBatch Accuracy: 90.67. Validation Accuracy: 89.34\n",
      "Epoch: 5000. Loss: 0.72. MiniBatch Accuracy: 86.67. Validation Accuracy: 89.34\n",
      "Epoch: 5100. Loss: 0.66. MiniBatch Accuracy: 90.00. Validation Accuracy: 89.25\n",
      "Epoch: 5200. Loss: 0.75. MiniBatch Accuracy: 87.33. Validation Accuracy: 89.35\n",
      "Epoch: 5300. Loss: 0.64. MiniBatch Accuracy: 90.67. Validation Accuracy: 89.33\n",
      "Epoch: 5400. Loss: 0.56. MiniBatch Accuracy: 92.00. Validation Accuracy: 89.34\n",
      "Epoch: 5500. Loss: 0.57. MiniBatch Accuracy: 91.33. Validation Accuracy: 89.44\n",
      "Epoch: 5600. Loss: 0.64. MiniBatch Accuracy: 90.00. Validation Accuracy: 89.36\n",
      "Epoch: 5700. Loss: 0.57. MiniBatch Accuracy: 90.67. Validation Accuracy: 89.54\n",
      "Epoch: 5800. Loss: 0.52. MiniBatch Accuracy: 90.00. Validation Accuracy: 89.48\n",
      "Epoch: 5900. Loss: 0.62. MiniBatch Accuracy: 89.33. Validation Accuracy: 89.49\n",
      "Epoch: 6000. Loss: 0.48. MiniBatch Accuracy: 93.33. Validation Accuracy: 89.69\n",
      "Epoch: 6100. Loss: 0.57. MiniBatch Accuracy: 90.67. Validation Accuracy: 89.43\n",
      "Epoch: 6200. Loss: 0.55. MiniBatch Accuracy: 93.33. Validation Accuracy: 89.56\n",
      "Epoch: 6300. Loss: 0.73. MiniBatch Accuracy: 90.00. Validation Accuracy: 89.63\n",
      "Epoch: 6400. Loss: 0.52. MiniBatch Accuracy: 92.67. Validation Accuracy: 89.42\n",
      "Epoch: 6500. Loss: 0.63. MiniBatch Accuracy: 90.00. Validation Accuracy: 89.75\n",
      "Epoch: 6600. Loss: 0.67. MiniBatch Accuracy: 88.67. Validation Accuracy: 89.62\n",
      "Epoch: 6700. Loss: 0.63. MiniBatch Accuracy: 88.00. Validation Accuracy: 89.52\n",
      "Epoch: 6800. Loss: 0.57. MiniBatch Accuracy: 88.67. Validation Accuracy: 89.67\n",
      "Epoch: 6900. Loss: 0.55. MiniBatch Accuracy: 92.00. Validation Accuracy: 89.73\n",
      "Epoch: 7000. Loss: 0.63. MiniBatch Accuracy: 88.67. Validation Accuracy: 89.75\n",
      "Epoch: 7100. Loss: 0.60. MiniBatch Accuracy: 87.33. Validation Accuracy: 89.82\n",
      "Epoch: 7200. Loss: 0.61. MiniBatch Accuracy: 88.67. Validation Accuracy: 89.83\n",
      "Epoch: 7300. Loss: 0.52. MiniBatch Accuracy: 92.00. Validation Accuracy: 89.64\n",
      "Epoch: 7400. Loss: 0.57. MiniBatch Accuracy: 88.00. Validation Accuracy: 89.81\n",
      "Epoch: 7500. Loss: 0.48. MiniBatch Accuracy: 93.33. Validation Accuracy: 89.88\n",
      "Epoch: 7600. Loss: 0.47. MiniBatch Accuracy: 92.00. Validation Accuracy: 89.91\n",
      "Epoch: 7700. Loss: 0.53. MiniBatch Accuracy: 90.67. Validation Accuracy: 89.87\n",
      "Epoch: 7800. Loss: 0.56. MiniBatch Accuracy: 90.00. Validation Accuracy: 89.88\n",
      "Epoch: 7900. Loss: 0.51. MiniBatch Accuracy: 92.67. Validation Accuracy: 89.95\n",
      "Epoch: 8000. Loss: 0.54. MiniBatch Accuracy: 88.67. Validation Accuracy: 90.01\n",
      "Epoch: 8100. Loss: 0.66. MiniBatch Accuracy: 84.00. Validation Accuracy: 90.17\n",
      "Epoch: 8200. Loss: 0.54. MiniBatch Accuracy: 94.00. Validation Accuracy: 90.07\n",
      "Epoch: 8300. Loss: 0.65. MiniBatch Accuracy: 86.67. Validation Accuracy: 90.01\n",
      "Epoch: 8400. Loss: 0.65. MiniBatch Accuracy: 86.00. Validation Accuracy: 90.14\n",
      "Epoch: 8500. Loss: 0.52. MiniBatch Accuracy: 90.00. Validation Accuracy: 90.16\n",
      "Epoch: 8600. Loss: 0.54. MiniBatch Accuracy: 88.67. Validation Accuracy: 90.20\n",
      "Epoch: 8700. Loss: 0.65. MiniBatch Accuracy: 86.67. Validation Accuracy: 90.05\n",
      "Epoch: 8800. Loss: 0.50. MiniBatch Accuracy: 90.00. Validation Accuracy: 90.11\n",
      "Epoch: 8900. Loss: 0.61. MiniBatch Accuracy: 88.00. Validation Accuracy: 90.01\n",
      "Epoch: 9000. Loss: 0.48. MiniBatch Accuracy: 90.67. Validation Accuracy: 90.23\n",
      "Epoch: 9100. Loss: 0.47. MiniBatch Accuracy: 91.33. Validation Accuracy: 90.12\n",
      "Epoch: 9200. Loss: 0.52. MiniBatch Accuracy: 90.67. Validation Accuracy: 90.24\n",
      "Epoch: 9300. Loss: 0.57. MiniBatch Accuracy: 92.00. Validation Accuracy: 90.20\n",
      "Epoch: 9400. Loss: 0.43. MiniBatch Accuracy: 94.67. Validation Accuracy: 90.24\n",
      "Epoch: 9500. Loss: 0.55. MiniBatch Accuracy: 87.33. Validation Accuracy: 90.41\n",
      "Epoch: 9600. Loss: 0.59. MiniBatch Accuracy: 88.67. Validation Accuracy: 90.34\n",
      "Epoch: 9700. Loss: 0.64. MiniBatch Accuracy: 85.33. Validation Accuracy: 90.33\n",
      "Epoch: 9800. Loss: 0.59. MiniBatch Accuracy: 87.33. Validation Accuracy: 90.44\n",
      "Epoch: 9900. Loss: 0.64. MiniBatch Accuracy: 87.33. Validation Accuracy: 90.22\n",
      "Epoch: 10000. Loss: 0.57. MiniBatch Accuracy: 88.67. Validation Accuracy: 90.09\n",
      "Test Accuracy: 95.4\n"
     ]
    }
   ],
   "source": [
    "epochs = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(epochs):\n",
    "        subset = np.random.choice(train_dataset.shape[0], size=batch_size, replace=False)\n",
    "        feed_dict = {data.X_train: train_dataset[subset], data.y_train: train_labels[subset]}\n",
    "        _, l, minibatch_prediction = sess.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "        if (epoch % 100 == 0):\n",
    "            print('Epoch: {}. Loss: {:.2f}. MiniBatch Accuracy: {:.2f}. Validation Accuracy: {:.2f}'.format(\n",
    "                epoch,\n",
    "                l, \n",
    "                accuracy(minibatch_prediction, train_labels[subset]), \n",
    "                accuracy(validation_predictions.eval(), valid_labels)))\n",
    "    print('Test Accuracy: {}'.format(accuracy(test_predictions.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
